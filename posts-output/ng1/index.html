<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <meta charset="utf-8"/>
    <title>Gowder&#39;s Tech Blog (!): Lecture 2 of Andrew Ng&#39;s mathier ML course</title>
    <link rel="canonical" href="http://paultopia.github.io/posts-output/ng1/">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href='//fonts.googleapis.com/css?family=Alegreya:400italic,700italic,400,700' rel='stylesheet'
          type='text/css'>
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/css/bootstrap.min.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/styles/default.min.css">
    <link href="/css/screen.css" rel="stylesheet" type="text/css" />
</head>
<body>


<nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Gowder&#39;s Tech Blog (!)</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="/">Home</a></li>
                <li
                ><a href="/archives/">Archives</a></li>
                
                <li
                >
                <a href="/pages-output/about/">About</a>
                </li>
                
                <li><a href="/feed.xml">RSS</a></li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
                        More <span class="caret"></span></a>
                    <ul class="dropdown-menu" role="menu">
                        <li class="dropdown-header">Links</li>
                        <li><a href="https://gowder.io">Moi</a></li>
                        <li><a href="https://github.com/paultopia">Github</a></li>
                        <li><a href="https://twitter.com/PaulGowder">Twitter</a></li>
                        <li><a href="http://www.linkedin.com/in/paulgowder">LinkedIn</a></li>
                        

                        
                        <li class="divider"></li>
                        <li class="dropdown-header">Recent Posts</li>
                        
                        <li><a href="/posts-output/jupyter-experiment-with-image/">Experimental post: jupyter notebook --&gt; cryogen post</a></li>
                        
                        <li><a href="/posts-output/jsoup-is-awesome/">For Clojure Webscraping, Try Jsoup!</a></li>
                        
                        <li><a href="/posts-output/clojure-java/">Understanding Java for Clojurists, side-by-side</a></li>
                        
                        

                        
                        <li class="divider"></li>
                        <li class="dropdown-header">Tags</li>
                        
                        <li><a href="/tags-output/game theory/">game theory</a></li>
                        
                        <li><a href="/tags-output/latex/">latex</a></li>
                        
                        <li><a href="/tags-output/cryogen/">cryogen</a></li>
                        
                        <li><a href="/tags-output/testing/">testing</a></li>
                        
                        <li><a href="/tags-output/clojure/">clojure</a></li>
                        
                        <li><a href="/tags-output/web/">web</a></li>
                        
                        <li><a href="/tags-output/machine-learning/">machine-learning</a></li>
                        
                        <li><a href="/tags-output/browsers/">browsers</a></li>
                        
                        <li><a href="/tags-output/postgresql/">postgresql</a></li>
                        
                        <li><a href="/tags-output/css/">css</a></li>
                        
                        <li><a href="/tags-output/java/">java</a></li>
                        
                        <li><a href="/tags-output/unicode/">unicode</a></li>
                        
                        <li><a href="/tags-output/osx/">osx</a></li>
                        
                        <li><a href="/tags-output/text-mining/">text-mining</a></li>
                        
                        <li><a href="/tags-output/haskell/">haskell</a></li>
                        
                        <li><a href="/tags-output/machinelearning/">machinelearning</a></li>
                        
                        <li><a href="/tags-output/flexbox/">flexbox</a></li>
                        
                        <li><a href="/tags-output/math/">math</a></li>
                        
                        <li><a href="/tags-output/shell/">shell</a></li>
                        
                        <li><a href="/tags-output/heroku/">heroku</a></li>
                        
                        <li><a href="/tags-output/blog/">blog</a></li>
                        
                        <li><a href="/tags-output/latin-1/">latin-1</a></li>
                        
                        <li><a href="/tags-output/reagent/">reagent</a></li>
                        
                        <li><a href="/tags-output/emacs/">emacs</a></li>
                        
                        <li><a href="/tags-output/datascience/">datascience</a></li>
                        
                        <li><a href="/tags-output/package management/">package management</a></li>
                        
                        <li><a href="/tags-output/git/">git</a></li>
                        
                        <li><a href="/tags-output/spacemacs/">spacemacs</a></li>
                        
                        <li><a href="/tags-output/functional programming/">functional programming</a></li>
                        
                        <li><a href="/tags-output/templates/">templates</a></li>
                        
                        <li><a href="/tags-output/r/">r</a></li>
                        
                        <li><a href="/tags-output/utf-8/">utf-8</a></li>
                        
                        <li><a href="/tags-output/object-oriented programming/">object-oriented programming</a></li>
                        
                        <li><a href="/tags-output/debugging/">debugging</a></li>
                        
                        <li><a href="/tags-output/meta/">meta</a></li>
                        
                        <li><a href="/tags-output/flask/">flask</a></li>
                        
                        <li><a href="/tags-output/markdown/">markdown</a></li>
                        
                        <li><a href="/tags-output/homebrew/">homebrew</a></li>
                        
                        <li><a href="/tags-output/devcards/">devcards</a></li>
                        
                        <li><a href="/tags-output/javascript/">javascript</a></li>
                        
                        <li><a href="/tags-output/system/">system</a></li>
                        
                        <li><a href="/tags-output/python/">python</a></li>
                        
                        <li><a href="/tags-output/webscraping/">webscraping</a></li>
                        
                        <li><a href="/tags-output/performance/">performance</a></li>
                        
                        <li><a href="/tags-output/closures/">closures</a></li>
                        
                        <li><a href="/tags-output/react/">react</a></li>
                        
                        <li><a href="/tags-output/data/">data</a></li>
                        
                        <li><a href="/tags-output/machine learning/">machine learning</a></li>
                        
                        <li><a href="/tags-output/algorithms/">algorithms</a></li>
                        
                        <li><a href="/tags-output/numpy/">numpy</a></li>
                        
                        <li><a href="/tags-output/clojurescript/">clojurescript</a></li>
                        
                        <li><a href="/tags-output/mustache/">mustache</a></li>
                        
                        <li><a href="/tags-output/test/">test</a></li>
                        
                        <li><a href="/tags-output/trees/">trees</a></li>
                        
                        
                    </ul>
                </li>
            </ul>
        </div><!--/.nav-collapse -->
    </div><!--/.container-fluid -->
</nav>


<div class="container">


    <div class="row">
        <div class="col-lg-12">
            <div id="content">
                
<div id="post">
    <div class="post-header">
    <div id="post-meta" class="row">
        <div class="col-lg-6">July 7, 2017</div>
        
    </div>
    <h2>Lecture 2 of Andrew Ng&#39;s mathier ML course</h2>
</div>
<div>
    
     <p>One of the things I'm doing at <a href='https://www.recurse.com/'>RC</a> is working through <a href='https://see.stanford.edu/Course/CS229/54'>the mathier version</a> of Andrew Ng's famous machine learning course. Here are my notes from the first substantive lecture (lecture 2). </p><p>n.b. the math is all in code blocks because the markdown processor screws with underscores and carets otherwise, and mathjax can't handle that.  This is making me insane and I might actually write some kind of post-processor to jerk around the generated html files to fix this, but it'll have to do for now.</p><h2><a name="lecture&#95;2"></a>Lecture 2</h2><p>Linear regression, mostly stuff I've already seen a bunch of times, but with the derivations using weirder linear algebra than usual, and with more gradient descent and less closed-form solutions.</p><p>His notation: </p><p>m = number of training examples</p><p>x = features</p><p>y = output (labels/target)</p><p>(x, y) = training example</p><p>superscript i for indexing over examples.</p><p>h for model hypothesis &mdash; it's the function mapping x->y </p><p>representing weights as $\theta$</p><p>n is the number of features</p><p>(Aaah, why not use n for the number of observations like scientists do?  Why can we not have consistent naming and notation in this world? Also, at some point when I was in grad school, we made a Stanford polisci t-shirt that had the slogan "it's not the size of your n, it's how you use it." Where did that go? I miss that shirt.)</p><p><code>$J&#40;\theta&#41;$</code> is the sum of squared errors / 2.  (Apparently, from discussion afterward with mathier people, these get divided by 2 in order to make differentiation cleaner.)</p><p>Gradient descent algorithm:</p><pre><code class="nohighlight"> $$\theta&#95;i := \theta&#95;i - \partial \frac{\partial}{\partial \theta} J&#40;\theta&#41;$$
</code></pre><p>that is, update the ith weight by subtracting the partial derivative of the cost (J(θ)) with respect to the ith weight (30:29 in video 2).</p><p>Calculus reminder: that <a href='https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives'>partial derivative</a> is essentially the magnitude of the change in the cost given an epsilon change in the weight at the given value of θ. </p><p>This <a href='https://www.youtube.com/watch?v=i94OvYb6noo'>lecture</a> (suggested by a batch-mate) is also good on the gradients:</p><p>(Ng uses <code>:=</code> to mean "update the variable on the left by the algorithm on the right")</p><p>Ultimately, it's just taking a step in the direction of the (local) minimum error.</p><p>After doing the calculus, that turns into: </p><pre><code class="nohighlight">$$\theta&#95;i : = \theta&#95;i - \alpha &#40;h&#95;{\theta}&#40;x&#41; - y&#41; \cdot x&#95;i$$
</code></pre><p>where alpha is the learning rate (a model parameter). This is for one training example.</p><p>which is super convenient, since <code>$&#40;h&#95;{\theta}&#40;x&#41; - y&#41;$</code> is just the straightforward error at a given step.</p><h3><a name="batch&#95;gradient&#95;descent"></a>Batch gradient descent</h3><p>For m training examples, the algorithm just sums the error over the training examples, i.e.,</p><pre><code class="nohighlight">$$\theta&#95;i : = \theta&#95;i - \alpha \sum&#95;{j=1}&#94;{m} &#40;h&#95;{\theta}&#40;x&#94;{&#40;j&#41;}&#41; - y&#94;{&#40;j&#41;}&#41; \cdot x&#95;i&#94;{&#40;j&#41;}$$ 
</code></pre><p>and then repeat until convergence</p><p>For OLS, there's only one global minimum, it's just a quadratic function that (therefore?) is "bow-shaped" (I've never understood these visual analogies on functions, but I take it that this is a good thing, and maybe means the same thing as convex??  Convexity is the property we discussed in our little group afterwards, so probably.  Note to self, really need to understand convexity and what it entails lots better.), and so no nasty local minima.</p><p>(Terminology reminder: gradient = derivative. <a href='https://math.stackexchange.com/questions/1519367/difference-between-gradient-and-jacobian'>Essentially</a>.)</p><p>It turns out that this computation is the direction of steepest descent, for reasons that Ng doesn't feel like proving. </p><p>This is called "batch gradient descent" because at every step of gradient descent you look at the whole dataset, perform a sum over m training examples.</p><p>That's problematic if you have a ton of training examples.  So there's an alternative:</p><p><strong>Stochastic Gradient Descent</strong> (a.k.a. "incremental descent")</p><p>repeat until convergence: <pre><code>for j = 1 to m: 
    perform an update using just the jth training example &#40;for each i&#41; 
 </code></pre></p><p>that update is just<br /></p><pre><code class="nohighlight"> $$\theta&#95;i : = \theta&#95;i - \alpha &#40;h&#95;{\theta}&#40;x&#94;{&#40;j&#41;}&#41; - y&#94;{&#40;j&#41;}&#41; \cdot x&#95;i&#94;{&#40;j&#41;}$$
</code></pre><p>in practice, this tends to go rather faster for large datasets. It doesn't actually converge exactly to the global minimum, but they tend to wander close to it. </p><p>Question I had: what's stochastic about this? It's not like it's actually randomly sampling the data or anything. In discussion afterward, someone said that it's called stochastic because it's based on the idea that the expectation of the update on a single observation is the same as the expectation on the update on the whole thing, which makes sense well enough to me.</p><h3><a name="closed&#95;form&#95;solution&#95;of&#95;theta"></a>Closed form solution of theta</h3><p>More new notation, for matrix derivatives. I mostly let this go by, because I feel like I've learned this derivation once already, courtesy of grad school, and I don't feel the need to do it again with different linear algebra. But reference: <a href='https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf'>part 1 of his lecture notes</a> on pg. 8.</p><p>A few points of interest: explanation of the stuff in the notes:</p><p>delta J, J is a function of vector parameters theta, recall. The derivative of J with respect to theta is itself a vector of partial derivatives, a n+1 dimensional vector. So then we can rewrite the batch gradient example as theta (not subscripted&mdash;it's the whole thing, update the whole paramerer) minus that big gradient, i.e., <code>$\theta := \alpha \nabla&#95;\theta J$</code> &ndash;and all of those quantities are N+1dimensional vectors. (except alpha, obvs)</p><p>Definition that feel like a bit of linear algebra I skipped: if A is a square matrix, the <em>trace</em> of A is the sum of A's diagonal elements. <code>$tr A = \sum&#95;{i=1}&#94;n A&#95;{ii}$</code> Which sounds like skipped-over linear algebra to me.</p><p>Ultimately this leads to the classic closed form solution to OLS, which shows up on pg. 11 of part 1 of lecture notes. </p><p>Also might be worth noting (from video at 1:00) that the "design matrix" is a matrix that has the training examples input values on the rows.  In notes and on chalkboards, there's a very confusing notation with dashes and an unexplained superscript with a  T in it... but I take it that the first row is the vector of features for first training example, second row is for second draining example (from video at 1:01).</p><p>Then design matrix multiplied by theta vector is just the hypotheses for a given set of weights.  And the error is going to be elementwise subtracting the elements of the y vector (label vector, which gets an arrow over it in the notes like <code>$\overrightarrow{y}$</code>).</p><p>Anyway, classic closed form: </p><pre><code class="nohighlight"> $$\theta = &#40;X&#94;TX&#41;&#94;{-1}X&#94;T\overrightarrow{y}$$
</code></pre><p>This is our old friend OLS. Hello OLS. You're also <a href='https://github.com/paultopia/browser-stats/blob/master/statspop/src/statspop/math/regression.cljs#L15'>enjoyably easy to implement in clojurescript</a>.</p><p>Note at the end of the lecture in response to a student question: usually, if <code>$X&#94;TX$</code> isn't invertible, it's because you've got dependent features in there, like repeating the same feature twice or something. (or linear combination, I take it? Standard OLS blow-up...)</p><p>that's it!</p><p>(note to self for future: to get mathml and highlight.js and this markdown parser to play nicely together, math has to be in code blocks, but those code blocks have to be either inline or they have to get the <code>nohighlight</code> class; in the latter case there apparently needs to be whitespace after the class declaration on the code block or for some mysterious reason it'll get a bunch of other classes instead, god only knows why.)</p>
</div>

<div id="post-tags">
    <b>Tags: </b>
    
    <a href="/tags-output/math/">math</a>
    
    <a href="/tags-output/meta/">meta</a>
    
    <a href="/tags-output/test/">test</a>
    
</div>


    <div id="prev-next">
        
        <a href="/posts-output/haskell-debug-bgd/">&laquo; A Debugging Trek, and: (naive) Batch Gradient Descent in Haskell</a>
        
        
        <a class="right" href="/posts-output/ng3/">Mathy Ng Lecture 4: Newton&#39;s Method, Exponential Family Distributions, GLMs &raquo;</a>
        
    </div>

    


</div>

            </div>
        </div>
    </div>
    <footer>
        <p>Copyright &copy; 2016-2018 <a href="https://gowder.io">Paul Gowder</a>.</p>

        <p> All original content licensed under a <a rel="license" href="http://creativecoa rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a> mmons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. </p>

        <p class="rc-scout"></p>
    </footer>
</div>
<script src="//code.jquery.com/jquery-1.11.0.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/js/bootstrap.min.js"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- HERE ARE A BUNCH OF PAUL CUSTOMIZATIONS -->
<!-- try to get heavy klipse stuff loaded only if post has executable content -->


<!-- ditto mathjax -->

<script type="text/x-mathjax-config">
 MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']],
                               displayMath: [['$$','$$']],
                               processEscapes: true,
                               skipTags: ["script","noscript","style","textarea"]
 }});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



<script async defer src="https://www.recurse-scout.com/loader.js?t=883fcbc53dcca6e2fc6b228efe240125"></script>
</body>
</html>

