<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <meta charset="utf-8"/>
    <title>Gowder&#39;s Tech Blog (!): Lecture 3 of Andrew Ng&#39;s mathier ML course</title>
    <link rel="canonical" href="http://paultopia.github.io/posts-output/ng2/">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href='//fonts.googleapis.com/css?family=Alegreya:400italic,700italic,400,700' rel='stylesheet'
          type='text/css'>
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/css/bootstrap.min.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/styles/default.min.css">
    <link href="/css/screen.css" rel="stylesheet" type="text/css" />
</head>
<body>


<nav class="navbar navbar-default">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Gowder&#39;s Tech Blog (!)</a>
        </div>
        <div id="navbar" class="navbar-collapse collapse">
            <ul class="nav navbar-nav navbar-right">
                <li ><a href="/">Home</a></li>
                <li
                ><a href="/archives/">Archives</a></li>
                
                <li
                >
                <a href="/pages-output/about/">About</a>
                </li>
                
                <li><a href="/feed.xml">RSS</a></li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
                        More <span class="caret"></span></a>
                    <ul class="dropdown-menu" role="menu">
                        <li class="dropdown-header">Links</li>
                        <li><a href="https://gowder.io">Moi</a></li>
                        <li><a href="https://github.com/paultopia">Github</a></li>
                        <li><a href="https://twitter.com/PaulGowder">Twitter</a></li>
                        <li><a href="http://www.linkedin.com/in/paulgowder">LinkedIn</a></li>
                        

                        
                        <li class="divider"></li>
                        <li class="dropdown-header">Recent Posts</li>
                        
                        <li><a href="/posts-output/jupyter-experiment-with-image/">Experimental post: jupyter notebook --&gt; cryogen post</a></li>
                        
                        <li><a href="/posts-output/jsoup-is-awesome/">For Clojure Webscraping, Try Jsoup!</a></li>
                        
                        <li><a href="/posts-output/clojure-java/">Understanding Java for Clojurists, side-by-side</a></li>
                        
                        

                        
                        <li class="divider"></li>
                        <li class="dropdown-header">Tags</li>
                        
                        <li><a href="/tags-output/game theory/">game theory</a></li>
                        
                        <li><a href="/tags-output/latex/">latex</a></li>
                        
                        <li><a href="/tags-output/cryogen/">cryogen</a></li>
                        
                        <li><a href="/tags-output/testing/">testing</a></li>
                        
                        <li><a href="/tags-output/clojure/">clojure</a></li>
                        
                        <li><a href="/tags-output/web/">web</a></li>
                        
                        <li><a href="/tags-output/machine-learning/">machine-learning</a></li>
                        
                        <li><a href="/tags-output/browsers/">browsers</a></li>
                        
                        <li><a href="/tags-output/postgresql/">postgresql</a></li>
                        
                        <li><a href="/tags-output/css/">css</a></li>
                        
                        <li><a href="/tags-output/java/">java</a></li>
                        
                        <li><a href="/tags-output/unicode/">unicode</a></li>
                        
                        <li><a href="/tags-output/osx/">osx</a></li>
                        
                        <li><a href="/tags-output/text-mining/">text-mining</a></li>
                        
                        <li><a href="/tags-output/haskell/">haskell</a></li>
                        
                        <li><a href="/tags-output/machinelearning/">machinelearning</a></li>
                        
                        <li><a href="/tags-output/flexbox/">flexbox</a></li>
                        
                        <li><a href="/tags-output/math/">math</a></li>
                        
                        <li><a href="/tags-output/shell/">shell</a></li>
                        
                        <li><a href="/tags-output/heroku/">heroku</a></li>
                        
                        <li><a href="/tags-output/blog/">blog</a></li>
                        
                        <li><a href="/tags-output/latin-1/">latin-1</a></li>
                        
                        <li><a href="/tags-output/reagent/">reagent</a></li>
                        
                        <li><a href="/tags-output/emacs/">emacs</a></li>
                        
                        <li><a href="/tags-output/datascience/">datascience</a></li>
                        
                        <li><a href="/tags-output/package management/">package management</a></li>
                        
                        <li><a href="/tags-output/git/">git</a></li>
                        
                        <li><a href="/tags-output/spacemacs/">spacemacs</a></li>
                        
                        <li><a href="/tags-output/functional programming/">functional programming</a></li>
                        
                        <li><a href="/tags-output/templates/">templates</a></li>
                        
                        <li><a href="/tags-output/r/">r</a></li>
                        
                        <li><a href="/tags-output/utf-8/">utf-8</a></li>
                        
                        <li><a href="/tags-output/object-oriented programming/">object-oriented programming</a></li>
                        
                        <li><a href="/tags-output/debugging/">debugging</a></li>
                        
                        <li><a href="/tags-output/meta/">meta</a></li>
                        
                        <li><a href="/tags-output/flask/">flask</a></li>
                        
                        <li><a href="/tags-output/markdown/">markdown</a></li>
                        
                        <li><a href="/tags-output/homebrew/">homebrew</a></li>
                        
                        <li><a href="/tags-output/devcards/">devcards</a></li>
                        
                        <li><a href="/tags-output/javascript/">javascript</a></li>
                        
                        <li><a href="/tags-output/system/">system</a></li>
                        
                        <li><a href="/tags-output/python/">python</a></li>
                        
                        <li><a href="/tags-output/webscraping/">webscraping</a></li>
                        
                        <li><a href="/tags-output/performance/">performance</a></li>
                        
                        <li><a href="/tags-output/closures/">closures</a></li>
                        
                        <li><a href="/tags-output/react/">react</a></li>
                        
                        <li><a href="/tags-output/data/">data</a></li>
                        
                        <li><a href="/tags-output/machine learning/">machine learning</a></li>
                        
                        <li><a href="/tags-output/algorithms/">algorithms</a></li>
                        
                        <li><a href="/tags-output/numpy/">numpy</a></li>
                        
                        <li><a href="/tags-output/clojurescript/">clojurescript</a></li>
                        
                        <li><a href="/tags-output/mustache/">mustache</a></li>
                        
                        <li><a href="/tags-output/test/">test</a></li>
                        
                        <li><a href="/tags-output/trees/">trees</a></li>
                        
                        
                    </ul>
                </li>
            </ul>
        </div><!--/.nav-collapse -->
    </div><!--/.container-fluid -->
</nav>


<div class="container">


    <div class="row">
        <div class="col-lg-12">
            <div id="content">
                
<div id="post">
    <div class="post-header">
    <div id="post-meta" class="row">
        <div class="col-lg-6">July 7, 2017</div>
        
    </div>
    <h2>Lecture 3 of Andrew Ng&#39;s mathier ML course</h2>
</div>
<div>
    
      <h2><a name="lecture&#95;3&#95;&ndash;&#95;locally&#95;weighted&#95;regression"></a>Lecture 3 &ndash; locally weighted regression</h2><p>Nonparametric algorithms reduce "the need to choose features very carefully" (I guess that makes sense if you think of features as mathematical transformations on stuff observed rather than stuff observed in general... a nonparemetric algorithm surely can't avoid the fact that you left something off, though I guess it can help avoid the fact that you threw a bunch of extra stuff in...)</p><p>Formal definition of a nonparametric algorithm is an algorithm where the number of parameters grows with m.  Which also means it needs to hold onto the entire training set even after training. As a student said in questions, "it's like you're not even really building a model at all." You just fit for every training example. (This seems really expensive!!)</p><h3><a name="locally&#95;weighted&#95;regression&#95;(loess/lowss)"></a>Locally weighted regression (loess/lowss)</h3><p>Concept: consider a value for x, the vector of features, of a single observation. To make a prediction with OLS, we'd find the vector of weights (parameters) <code>$\theta$</code> s.t. they minimize the cost function, then return <code>$\theta&#94;tx$</code> as prediction.</p><p>For loess, we'd take a region around x, and work on the subset of data around there. So, geometrically, rather than predicting y based a line fitted to the entire dataset, predicts y based a line fitted to a subset of the dataset around x.</p><p>formally, in loess we fit <code>$\theta$</code> to minimize a weighted version of the same loss function we use in OLS, where the weights are chosen such that we give more weight to training examples closer to what we're trying to predict. </p><p>i.e., OLS minimizes: </p><pre><code class="nohighlight"> $$\sum&#95;i&#40;y&#94;{&#40;i&#41;}-\theta&#94;Tx&#94;{&#40;i&#41;}&#41;&#94;2$$ 
</code></pre><p>while loess minimizes:</p><pre><code class="nohighlight"> $$\sum&#95;iw&#94;{&#40;i&#41;}&#40;y&#94;{&#40;i&#41;}-\theta&#94;Tx&#94;{&#40;i&#41;}&#41;&#94;2$$ 
</code></pre><p>the trick is in the definition of the weight function.  As I understand it it the fit is made at the time of prediction, so x without a subscript in the below is the value of the feature <em>for which you're trying to predict the output</em> (and that's why you have to keep your training data around even after training, as Ng noted earlier. Do you even train at all in advance? Maybe there's some optimization trick that allows you to pre-train something? Kinda doubting it from the "not even a model" chat above.).  So with that preamble, the weight function is </p><pre><code class="nohighlight"> $$w&#94;{&#40;i&#41;}=e&#94;{&#40;-\frac{&#40;x&#94;{&#40;i&#41;}-x&#41;&#94;2}{2}&#41;}$$ 
</code></pre><p>Actually, he said there are lots of possible weight functions, but the point is to have something that gets close to zero when <code>$x&#94;{&#40;i&#41;}$</code> is far from x and close to 1 when they're close together.  Which, obviously, this satisfies.</p><p>A more common form of the weight is</p><pre><code class="nohighlight"> $$w&#94;{&#40;i&#41;}=e&#94;{&#40;-\frac{&#40;x&#94;{&#40;i&#41;}-x&#41;&#94;2}{2\tau&#94;2}&#41;}$$ 
</code></pre><p>where tau is a "bandwidth parameter" that controls the rate at which the weighting function falls off with distance from x.</p><p>Another student question: this is indeed very costly, "every time you make a prediction you need to fit theta to your entire training set again." However, "turns out there are ways to make this much more efficient." He referred to <a href='http://www.ri.cmu.edu/pub_files/pub1/moore_andrew_1991_1/moore_andrew_1991_1.pdf'>Andrew Moore's kd-trees</a> as this method.</p><h3><a name="probabilistic&#95;interpretation&#95;of&#95;linear&#95;regression"></a>Probabilistic interpretation of linear regression</h3><p>Why are we minimizing the sum of squared error as opposed to the absolute value or something? Assumptions that make this work. (Oh boy, are we going to do BLUE again?  Might skim past this.)</p><p>First we "endow the least squares model with probabilistic semantics."</p><p>Yeah, this is the same stuff.  Assume y is a function of the model plus error, assume error is IID and distributed normally with mean zero, all the good social science stats stuff I already know. Then we do the standard probability and algebra and get the maximum likelihood estimator, which turns out to be the OLS cost function.  And that was like a whole class in grad school. The central limit theorem came up, as it does. All the good stuff. For his derivation, see pages 11-13 of <a href='https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf'>lecture notes 1</a>.</p><p>There is one useful notation note though.  Semicolon indicates not a random variable but as something we're trying to estimate in the world, i.e. this: </p><pre><code class="nohighlight"> $$P&#40;y&#94;{&#40;i&#41;}|x&#94;{&#40;i&#41;};\theta&#41;$$ 
</code></pre><p>indicates "the probability of <code>$y&#94;{&#40;i&#41;}$</code> conditioned on <code>$x&#94;{&#40;i&#41;}$</code>  as parameterized by <code>$\theta$</code> " while this:</p><pre><code class="nohighlight"> $$P&#40;y&#94;{&#40;i&#41;}|x&#94;{&#40;i&#41;},\theta&#41;$$ 
</code></pre><p>means "the probability of <code>$y&#94;{&#40;i&#41;}$</code> conditioned on <code>$x&#94;{&#40;i&#41;}$</code> and <code>$\theta$</code> " which is wrong, because theta isn't a random variable, it's a property of the world we're trying to estimate (in frequentist terms).</p><p>The conditional probability (the correct one) = the likelihood function, only y gets an arrow (to indicate it's observed?). So maximum likelihood.</p><h3><a name="classification"></a>Classification</h3><p>Started off with standard stuff, instead of choosing a linear function, we choose a nonlinear function.  And for logistic regression, that's the sigmoid function, a.k.a. the logistic function: </p><pre><code class="nohighlight"> $$h&#95;\theta&#40;x&#41; = \frac{1}{1 + e&#94;{-\theta&#94;Tx}}$$ 
</code></pre><p>One important point is that for logistic gradient descent we're actually ascending, that is, we're trying to maximize not minimize, so we add the gradient rather than subtract it.</p><p>Interestingly, it comes out to the same update rule with the sign swapped when the dust settles. It's not the same math because the function that generates the hypothesis is obviously different (the linear function vs the logistic function), but it has the same functional form.  Logistic gradient ascent update rule:</p><pre><code class="nohighlight"> $$\theta&#95;j : = \theta&#95;j + \alpha &#40;y&#94;{&#40;i&#41;} - h&#95;{\theta}&#40;x&#94;{&#40;i&#41;}&#41;&#41; \cdot x&#95;j&#94;{&#40;i&#41;}$$ 
</code></pre><p>Why does maximizing this work exactly?  It's just maximum likelihood again.  It turns out that for OLS, maximizing the likelihood function, after the math dust settles, is the same as minimizing the least squares cost function. (See notes pg. 13) But for logistic regression, when we <em>maximize</em> the log likelihood of the parameters, the gradient ascent that we use to directly maximize the likelihood just simplifies to the same form. </p><p>(An explanation of why turned up elsewhere: logistic regression likelihood is concave. page 11 of <a href='https://courses.cs.washington.edu/courses/cse547/16sp/slides/logistic-SGD.pdf'>this</a>)</p><h3><a name="digression&#95;on&#95;perceptrons"></a>digression on perceptrons</h3><p>a perceptron is just the same update rule but with this threshold function that maps positive values to 1 and negative values to 0 rather than logistic function's mapping of everything to the space from 0-1. </p><p>Hard to interpret perceptrons probabilistically though.</p>
</div>

<div id="post-tags">
    <b>Tags: </b>
    
    <a href="/tags-output/math/">math</a>
    
    <a href="/tags-output/meta/">meta</a>
    
    <a href="/tags-output/test/">test</a>
    
</div>


    <div id="prev-next">
        
        <a href="/posts-output/ng3/">&laquo; Mathy Ng Lecture 4: Newton&#39;s Method, Exponential Family Distributions, GLMs</a>
        
        
        <a class="right" href="/posts-output/game-as-tree/">Translating Game Theory Backward Induction into a Tree Algorithm &raquo;</a>
        
    </div>

    


</div>

            </div>
        </div>
    </div>
    <footer>
        <p>Copyright &copy; 2016-2018 <a href="https://gowder.io">Paul Gowder</a>.</p>

        <p> All original content licensed under a <a rel="license" href="http://creativecoa rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a> mmons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. </p>

        <p class="rc-scout"></p>
    </footer>
</div>
<script src="//code.jquery.com/jquery-1.11.0.min.js"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.0/js/bootstrap.min.js"></script>
<script src="/js/highlight.pack.js" type="text/javascript"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- HERE ARE A BUNCH OF PAUL CUSTOMIZATIONS -->
<!-- try to get heavy klipse stuff loaded only if post has executable content -->


<!-- ditto mathjax -->

<script type="text/x-mathjax-config">
 MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$']],
                               displayMath: [['$$','$$']],
                               processEscapes: true,
                               skipTags: ["script","noscript","style","textarea"]
 }});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



<script async defer src="https://www.recurse-scout.com/loader.js?t=883fcbc53dcca6e2fc6b228efe240125"></script>
</body>
</html>

